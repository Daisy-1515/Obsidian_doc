




## related work




[[Megatron-LM：Training Multi-Billion Parameter Language Models Using Model Parallelism Megatron-LM：利用模型并行性训练千亿参数语言模型]]
[[Flash Communication]]



## 数据并行和模型并行的区别
**数据并行**（Data Parallelism）和**模型并行**（Model Parallelism）是两种常用的分布式训练方法，旨在利用多个硬件加速器（如GPU）加速深度神经网络的训练。它们的主要区别在于**如何划分任务和分配计算资源**。
### 1. **数据并行（Data Parallelism）**

- **核心思想**：将训练数据划分为多个子集，并将每个子集分配给不同的计算设备进行处理。每个设备使用相同的模型进行训练，但训练的输入数据不同。
    
- **工作方式**：
    
    - 每个计算设备（比如GPU）负责计算它所分配的训练数据的梯度。
        
    - 每个设备的梯度计算独立进行，所有设备计算完成后，**聚合梯度**（通常使用All-Reduce或其他方法），然后同步更新模型的参数。
        
- **优点**：
    
    - 易于扩展到大量的计算设备，特别适合处理大规模数据集。
        
    - 模型结构在各个设备上完全相同，训练过程简单，容易实现。
        
- **缺点**：
    
    - 所有设备都需要存储模型的副本，因此内存使用量较大，适合模型比较小或者设备内存足够大的情况。
        

### 2. **模型并行（Model Parallelism）**

- **核心思想**：将一个模型的不同部分分配给不同的计算设备，每个设备负责计算模型的一部分。
    
- **工作方式**：
    
    - 模型被拆分成多个部分，每个计算设备负责计算模型的一部分（例如，一个设备计算前几层，另一个设备计算后几层）。
        
    - 各个设备之间需要频繁的通信以交换中间结果（例如，前一层的输出作为后一层的输入）。
        
- **优点**：
    
    - 适用于大模型，因为每个设备只需要存储和计算模型的一部分，可以解决单个设备内存不足的问题。
        
    - 可以处理更大的模型，特别是当模型的规模远超设备内存时。
        
- **缺点**：
    
    - 各个设备之间的通信开销较大，可能成为性能瓶颈。
        
    - 实现较为复杂，因为需要对模型进行拆分并确保设备之间的高效协作。
        

### 关键区别总结：

|特性|数据并行（Data Parallelism）|模型并行（Model Parallelism）|
|---|---|---|
|**任务划分**|数据划分，每个设备计算相同的模型但使用不同的数据|模型划分，每个设备计算不同的模型部分|
|**计算模型**|所有设备使用相同的模型副本|每个设备使用模型的不同部分|
|**通信开销**|主要集中在梯度同步（聚合）上|需要频繁的设备间通信来传递中间计算结果|
|**内存使用**|每个设备存储完整的模型副本|每个设备只存储模型的一部分|
|**适用场景**|数据量大，模型相对较小|模型非常大，超出了单个设备的内存限制|

这两种方法可以组合使用（**混合并行**），例如在每个设备上使用模型并行，在多个设备之间使用数据并行。